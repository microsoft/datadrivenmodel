{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "swiss-leeds",
   "metadata": {},
   "source": [
    "# DDM: Model Validation\n",
    "\n",
    "This notebook provides an overview of the tools built in the DDM for extracting predictions from your trained DDM and for evauating the performance of the DDM.\n",
    "\n",
    "---\n",
    "\n",
    "We utilize `hydra` to save the configuration of our datasets and our models. The default configuration is in the `conf/config.yaml` directory:\n",
    "\n",
    "```YAML\n",
    "defaults:\n",
    "  - data: cartpole_st1_at.yaml\n",
    "  - model: SVR.yaml\n",
    "  - simulator: gboost_cartpole.yaml\n",
    "```\n",
    "\n",
    "Note that the configuration file points to three additional configuration files for each component: the data, the model, and the simulator.\n",
    "\n",
    "While the configuration file already has default values specified you can override any element of the configuration file using the `overrides` option. For example, we override the data configuration to instead use the `yaml` file specified in `data/cartpole-st_at.yaml` and the model configuration to use the `yaml` file specified in `model/xgboost.yaml`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "graphic-falls",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\microsoft-datadrivenmodel\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-continent",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra.experimental import initialize, compose\n",
    "from model_loader import available_models\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from rich import print\n",
    "from rich.logging import RichHandler\n",
    "import copy\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(message)s\",\n",
    "    datefmt=\"[%X]\",\n",
    "    handlers=[RichHandler()]\n",
    ")\n",
    "logger = logging.getLogger(\"ddm_validation\")\n",
    "logger.setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-lincoln",
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize(config_path=\"../conf\", job_name=\"model_validation\")\n",
    "cfg = compose(config_name=\"config\", overrides=[\"data=cartpole_st_at\", \"model=xgboost\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-brief",
   "metadata": {},
   "source": [
    "## 1. Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "saved-boulder",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-grass",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"model\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "asian-monkey",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from yaml file\n",
    "input_cols = cfg['data']['inputs']\n",
    "output_cols = cfg['data']['outputs']\n",
    "augmented_cols = cfg['data']['augmented_cols']\n",
    "dataset_path = cfg['data']['path']\n",
    "iteration_order = cfg['data']['iteration_order']\n",
    "episode_col = cfg['data']['episode_col']\n",
    "iteration_col = cfg['data']['iteration_col']\n",
    "max_rows = cfg['data']['max_rows']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confirmed-corner",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"DATA STRUCTURE SELECTED:\")\n",
    "print(\" - input_cols:\", input_cols)\n",
    "print(\" - augmented_cols:\", augmented_cols)\n",
    "print(\" - output_cols:\", output_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-benjamin",
   "metadata": {},
   "source": [
    "##  2. Model Definition\n",
    "\n",
    "The `available_models` dictionary provides wrappers for the available models in this repository. We utilize `cfg[\"model\"]` to load and build the model specified in the `model.yaml` file.\n",
    "\n",
    "### Training Hyperparameters\n",
    "\n",
    "Every model has its own hyperparameters, specified through the `cfg[\"model\"][\"build_params\"]` dictionary, which can be modified directly in the dictionary below or through the `hydra` overrides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floating-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg[\"model\"][\"build_params\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-division",
   "metadata": {},
   "source": [
    "## 3. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-praise",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(config=cfg):\n",
    "\n",
    "    logger.info(f'Model type: {available_models[config[\"model\"][\"name\"]]}')\n",
    "    Model = available_models[config[\"model\"][\"name\"]]\n",
    "    model = Model()\n",
    "    logger.info(f\"Building model with parameters: {config}\")\n",
    "    model.build_model(\n",
    "        **config[\"model\"][\"build_params\"]\n",
    "    )\n",
    "    logger.info(f\"Loading data from {dataset_path}\")\n",
    "    global X, y\n",
    "    X, y = model.load_csv(\n",
    "        input_cols=input_cols,\n",
    "        output_cols=output_cols,\n",
    "        augm_cols=list(augmented_cols),\n",
    "        dataset_path=dataset_path,\n",
    "        iteration_order=iteration_order,\n",
    "        episode_col=episode_col,\n",
    "        iteration_col=iteration_col,\n",
    "        max_rows=max_rows,\n",
    "    )\n",
    "\n",
    "    logger.info(f\"Fitting model...\")\n",
    "    model.fit(X, y)\n",
    "    logger.info(f\"Model trained!\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-national",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_models(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "requested-triumph",
   "metadata": {},
   "source": [
    "### Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minute-blanket",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model(filename=cfg[\"model\"][\"saver\"][\"filename\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-pennsylvania",
   "metadata": {},
   "source": [
    "### Data Structure of Saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "expressed-effectiveness",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f\"Input_cols:  {model.features}\")\n",
    "logger.info(f\"Output_cols: {model.labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-episode",
   "metadata": {},
   "source": [
    "## 4. Model Evaluations\n",
    "\n",
    "We provide three methods for evaluating the errors of our trained models:\n",
    "\n",
    "1. Model predictive error: using a specified metric (such as RMSE) and a test set, we evaluate the metric on the test set.\n",
    "2. Visualization of per-iteration predictions vs a test set\n",
    "3. Visualization of sequential predictions vs a test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-criminal",
   "metadata": {},
   "source": [
    "### 4.1. Estimated Prediction Error\n",
    "#### 4.1.a. Estimated Prediction Error: Per-Iteration Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "breeding-editing",
   "metadata": {},
   "outputs": [],
   "source": [
    "from assessment_metrics_loader import available_metrics\n",
    "rms_error = available_metrics[\"root_mean_squared_error\"]\n",
    "ms_error = available_metrics[\"mean_squared_error\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "senior-audio",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = model.get_test_set(grouped_per_episode=False)\n",
    "\n",
    "per_iteration_eval_table = model.evaluate(rms_error, X_test, y_test, marginal=True)\n",
    "\n",
    "per_iteration_eval_table.rename(columns = {'score':'score_per_it'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amazing-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (per_iteration_eval_table[\"score_per_it\"] > 1.0).any():\n",
    "    logger.warn(\"Per-iteration assessment error is high. Please, review your model\")\n",
    "\n",
    "per_iteration_eval_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "responsible-order",
   "metadata": {},
   "source": [
    "#### 4.1.b. Estimated Prediction Error: Per-Iteration Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-graduation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of iterations to predict sequentially\n",
    "it_per_episode = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-tennessee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_grouped, y_test_grouped = model.get_test_set(grouped_per_episode=True)\n",
    "print(type(X_test_grouped))\n",
    "\n",
    "sequential_eval_table = model.evaluate_sequentially(\n",
    "    rms_error,\n",
    "    X_test_grouped,\n",
    "    y_test_grouped,\n",
    "    marginal=True,\n",
    "    it_per_episode=it_per_episode\n",
    ")\n",
    "\n",
    "sequential_eval_table.rename(columns = {'score':'seq_score'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minimal-section",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (sequential_eval_table[\"seq_score\"] > 1.0).any():\n",
    "    logger.warn(\"Sequentially analyzed assessment error is high. Please, review your model\")\n",
    "\n",
    "sequential_eval_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "young-adult",
   "metadata": {},
   "source": [
    "### 4.2. Per-Iteration Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "likely-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "action_feat = \"action_command\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-warehouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = model.get_test_set(grouped_per_episode=False)\n",
    "preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "express-multimedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results graphs\n",
    "label_count = np.shape(preds)[1]\n",
    "fig,axs = plt.subplots(3,2,)\n",
    "for i in range(label_count):\n",
    "    axs[i//2,i%2].set_title(f\"Per-iteration predictions: {model.labels[i]}\")\n",
    "    axs[i//2,i%2].plot(y_test[:,0], \"green\")\n",
    "    axs[i//2,i%2].plot(preds[:,i], \"brown\")\n",
    "    \n",
    "# select section to zoom in (for both action & episode visualization)\n",
    "sample_from = 0\n",
    "sample_to = 100\n",
    "\n",
    "# Plot action changes\n",
    "action_idx = model.features.index(action_feat)\n",
    "axs[(i+1)//2,(i+1)%2].set_title(f\"Plot [{sample_from}:{sample_to}] actions (ensure action is not stale)\")\n",
    "axs[(i+1)//2,(i+1)%2].plot(X[sample_from:sample_to,action_idx])\n",
    "    \n",
    "# Plot prediction graph -- with zoom on 100 iterations\n",
    "action_idx = model.features.index(action_feat)\n",
    "feat_idx = 0\n",
    "axs[(i+2)//2,(i+2)%2].set_title(f\"Plot [{sample_from}:{sample_to}] state preds: {model.labels[feat_idx]}\")\n",
    "axs[(i+2)//2,(i+2)%2].plot(y_test[sample_from:sample_to,feat_idx], \"green\")\n",
    "axs[(i+2)//2,(i+2)%2].plot(preds[sample_from:sample_to,feat_idx], \"brown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "existing-outline",
   "metadata": {},
   "source": [
    "### 4.3. Sequential Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-adapter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of iterations to predict sequentially\n",
    "# - taking the value defined above by default -\n",
    "it_per_episode = it_per_episode\n",
    "action_feat = action_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "communist-atlanta",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_grouped, y_test_grouped = model.get_test_set(grouped_per_episode=True)\n",
    "\n",
    "preds_sequentially, labels_sequentially = model.predict_sequentially(\n",
    "    X_test_grouped,\n",
    "    y_test_grouped,\n",
    "    it_per_episode=it_per_episode\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-cross",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results graphs\n",
    "label_count = np.shape(preds_sequentially)[1]\n",
    "fig,axs = plt.subplots(3,2,)\n",
    "for i in range(label_count):\n",
    "    axs[i//2,i%2].set_title(f\"Sequential predictions: {model.labels[i]}\")\n",
    "    axs[i//2,i%2].plot(labels_sequentially[:,0], \"green\")\n",
    "    axs[i//2,i%2].plot(preds_sequentially[:,i], \"brown\")\n",
    "    \n",
    "# select section to zoom in (for both action & episode visualization)\n",
    "sample_from = 0\n",
    "sample_to = 100\n",
    "\n",
    "# Plot action changes\n",
    "action_idx = model.features.index(action_feat)\n",
    "axs[(i+1)//2,(i+1)%2].set_title(f\"Plot [{sample_from}:{sample_to}] actions (ensure action is not stale)\")\n",
    "axs[(i+1)//2,(i+1)%2].plot(X[sample_from:sample_to,action_idx])\n",
    "    \n",
    "# Plot prediction graph -- with zoom on 100 iterations\n",
    "action_idx = model.features.index(action_feat)\n",
    "feat_idx = 0\n",
    "axs[(i+2)//2,(i+2)%2].set_title(f\"Plot [{sample_from}:{sample_to}] state preds: {model.labels[feat_idx]}\")\n",
    "axs[(i+2)//2,(i+2)%2].plot(labels_sequentially[sample_from:sample_to,feat_idx], \"green\")\n",
    "axs[(i+2)//2,(i+2)%2].plot(preds_sequentially[sample_from:sample_to,feat_idx], \"brown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secret-peter",
   "metadata": {},
   "source": [
    "## 5. Comparing Model Evaluations\n",
    "\n",
    "If you want to compare various models, you can use the following section to save them in between runs.\n",
    "\n",
    "1. Select appropriate \"model_name\" tag, and run this section\n",
    "2. Change config through \"config.yaml\" (located at 'conf' folder)\n",
    "3. Rerun from Model Build, until this section\n",
    "4. Define a new value for \"model_name\" tag, and run this section again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "running-offer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select model name, and feats to extract\n",
    "model_name = \"xgb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-cotton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# per-iteration & sequential scores\n",
    "model_per_it_scores = copy.deepcopy(per_iteration_eval_table)\n",
    "model_sequential_scores = copy.deepcopy(sequential_eval_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controversial-dynamics",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize models dictionary if it doesn't exist already\n",
    "if 'models_dict' not in locals():\n",
    "    models_dict = dict()\n",
    "\n",
    "# append tables to model using selected model name as key\n",
    "models_dict[model_name] = (model_per_it_scores, model_sequential_scores)\n",
    "\n",
    "# redefine column names if needed\n",
    "for model_name, score_tables in models_dict.items():\n",
    "    for score_table in score_tables:\n",
    "        for col_name in score_table.columns:\n",
    "            if \"score\" in col_name and model_name not in col_name:\n",
    "                score_table.rename(columns = {col_name:model_name+\"_\"+col_name}, inplace = True)\n",
    "\n",
    "# concatenate across all models\n",
    "all_scores = None\n",
    "for model_name, score_tables in models_dict.items():\n",
    "    for score_table in score_tables:\n",
    "        if all_scores is None:\n",
    "            all_scores = score_table\n",
    "        else:\n",
    "            all_scores = all_scores.merge(score_table,how='outer')\n",
    "\n",
    "all_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
